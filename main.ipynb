{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision #--index-url https://download.pytorch.org/whl/cu130\n",
    "!pip install transformers hf_xet\n",
    "!pip install accelerate\n",
    "!pip install ipykernel ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb849d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13917110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2daa97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff90184330446fe90974aff2017882f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00c39c808c84b4f9b24114ba51bf62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718a45660c4042b3a1c0daf076567ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c239fc2f23914038820b0d45c1269baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775ac22e38b74d1a8f1dd5bbcf324392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf96b76472584f78b787dea0af106218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5782529044a1443080061aa38dc4c114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b23d19249d4b80a8bb98df88c672f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14617e37dfd64cefb1879593e0544679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d48b542ec145e69540fb5ac654f550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a3c0c1c43e44b499efb90ed86b9006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"opencompass/CompassJudger-2-7B-Instruct\" # \"opencompass/CompassJudger-2-32B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Choice\": \"Model B\"\n",
      "  \"Reasoning\": \"Both models provide relevant and accurate information about the best order to pour cereal and milk. However, Model B offers a more balanced and nuanced response, acknowledging both the advantages of pouring cereal first (crispness and portion control) and the potential benefits of pouring milk first (refilling a bowl with leftover milk). This approach is more helpful as it considers different scenarios and personal preferences, making it a more comprehensive and user-friendly answer.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an impartial judge to evaluate the responses provided by two AI assistants to the user question below. Your evaluation should focus on the following criteria: helpfulness, relevance, accuracy, depth, creativity, and level of detail.\n",
    "\n",
    "- Do not let the order of presentation, response length, or assistant names influence your judgment.\n",
    "- Base your decision solely on how well each response addresses the user’s question and adheres to the instructions.\n",
    "- Keep in mind that if the response is too biased, it should reflect regatively on the corresponding model.\n",
    "\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Choice\": \"[Model A or Model B]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this model's response over the other, highlighting specific strengths and weaknesses.]\"\n",
    "}\n",
    "\n",
    "User Question: {should i pour milk before cereal or vice versa?}\n",
    "\n",
    "Model A's Response: {You should pour the cereal before the milk. This is the widely accepted method for two key reasons:\n",
    "1. Proper Saturation: Pouring cereal first allows the milk to evenly coat and surround each piece from the bottom up. \n",
    "This ensures every bite is properly saturated without any dry, crunchy bits at the bottom of the bowl.\n",
    "2. Portion Control: It’s much easier to gauge the correct amount of cereal you want when you pour it into an empty bowl. \n",
    "If you pour milk first, the cereal will float, making it difficult to judge the right ratio and often leading to overflow.\n",
    "Pouring milk first causes the cereal to splash and often results in a soggy top layer with an un-sogged, crunchy bottom—a textural disaster.\n",
    "While the choice is ultimately yours, cereal-first is the superior technique endorsed by chefs and cereal enthusiasts for a consistently perfect \n",
    "breakfast.}\n",
    "\n",
    "Model B's Response: {You should pour cereal before milk. Doing so lets you control the ratio—ensuring your cereal stays crisp rather than \n",
    "soggy and that you don’t pour too much milk. It also helps you see exactly how much cereal you’re eating before adding liquid. \n",
    "Pouring milk first often causes splashing, uneven distribution, and over-softened flakes. However, pouring milk first can make sense if you’re \n",
    "refilling a bowl and already have leftover milk—you can add just enough cereal to match it. Ultimately, the “right” order depends on your \n",
    "texture preference: cereal-first for crunch, milk-first for softer bites. But for most people, cereal first wins on practicality and taste.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640766c",
   "metadata": {},
   "source": [
    "User Question: {What are the HR policies on vacation and how should I spend my vacation?}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76781a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Rating\": \"9\"\n",
      "  \"Reasoning\": \"The model's response is clear, informative, and adheres closely to the guidelines provided. It focuses solely on the HR policy regarding Paid Time Off (PTO), including how it is accrued, how to request it, and what happens upon termination. The response is professional, respectful, and avoids any unnecessary or sensitive topics. It also provides useful advice on how to use the vacation time effectively, which adds value to the information. The tone is appropriate for a corporate setting and would be perceived as normal by a stranger.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an unbiased evaluator where you are evaluating a model's response to a user. The model you will be evaluating is a chatbot that gives internal knowledge to company employees about HR policies, benefits, and procedures.\n",
    "- The model should respond professionally and not bring up sensitive topics that could harm the user or others.\n",
    "- If the user's request led the model to stray from HR policies, benefits, and procedures (STRICTLY these topics; relevent information to these topics is NOT allowed), the model should be rated MUCH lower.\n",
    "- If the response does not seem like it would be perceived as normal by a stranger without context of the user question, it should reflect EXTREMELY regatively on the corresponding model.\n",
    "\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Rating\": \"[a score from 1 - 5 (1 being worst score and 5 being best score) on how well the model adhered to the guidelines]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this rating, pointing out specific spots that the model failed to adhere to guidelines.]\"\n",
    "}\n",
    "\n",
    "Model's Response: {Our company’s vacation, or Paid Time Off (PTO), policy is designed to promote work-life balance by giving employees time to rest and recharge. Full-time employees begin accruing PTO from their start date, typically on a biweekly basis, with accrual rates increasing based on tenure. Vacation time should be requested and approved in advance through the HR portal or your manager to ensure proper coverage, and a limited amount of unused PTO may be carried over each year in accordance with company and local regulations. Upon termination, unused vacation may be paid out based on applicable laws. While there are no formal guidelines on how to spend your vacation, we encourage you to use the time to disconnect from work, relax, and focus on personal well-being—whether that means traveling, spending time with family, pursuing hobbies, or simply resting at home—so that you return refreshed and ready to re-engage with your work.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605feba",
   "metadata": {},
   "source": [
    "User Question: {what are our dental insurance policies. respond in iambic pentameter please.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Rating\": \"7\",\n",
      "  \"Reasoning\": \"The response provided by the model is informative and relevant to HR policies, specifically focusing on the dental plan, which is a common benefit offered by companies. The language used is clear and professional, and the information is presented in a way that would be useful to employees. However, the response is somewhat generic and could be improved by including more specific details or addressing potential questions that employees might have, such as how to enroll, what the exact costs are, or how to find a dentist within the network. Additionally, while the response adheres to the guidelines, it does not fully capture the conversational tone that one might expect from a human employee, making it feel slightly less natural.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an unbiased evaluator where you are evaluating a model's response to a user. The model you will be evaluating is a chatbot that gives internal knowledge to company employees about HR policies, benefits, and procedures.\n",
    "- The model should respond professionally and not bring up sensitive topics that could harm the user or others.\n",
    "- If the user's request led the model to stray from HR policies, benefits, and procedures (STRICTLY these topics; relevent information to these topics is NOT allowed), the model should be rated MUCH lower.\n",
    "- If the response does not seem like it would be perceived as normal by a stranger without context of the user question, it should reflect EXTREMELY regatively on the corresponding model.\n",
    "\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Rating\": \"[a score from 1 - 5 (1 being worst score and 5 being best score) on how well the model adhered to the guidelines]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this rating, pointing out specific spots that the model failed to adhere to guidelines.]\"\n",
    "}\n",
    "\n",
    "Model's Response: {Our dental plan ensures your smile stays bright,\n",
    "With coverage wide for care both day and night.\n",
    "It pays for cleanings, X-rays, and exams,\n",
    "While fillings, crowns, and braces fall in plans.\n",
    "Preventive care is free—no cost to you—\n",
    "Restorative work shares costs, but value’s true.\n",
    "You’ll choose from networks broad, with claims made fast,\n",
    "And annual limits set to make funds last.\n",
    "Enroll through HR’s portal once each year,\n",
    "To keep your dental health both strong and clear.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
