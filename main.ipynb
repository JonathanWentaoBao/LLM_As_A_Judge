{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a465fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (0.24.0+cu130)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (2.9.0+cu130)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbao9\\anaconda3\\envs\\as_a_judge\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install transformers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb849d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2daa97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5063c9cdaf334180b2116f2ca93462b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbao9\\anaconda3\\envs\\as_a_judge\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jbao9\\.cache\\huggingface\\hub\\models--opencompass--CompassJudger-2-7B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2854d7b8d8af4060b225e668b471317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf7097376d042c8ac66ee5448a35df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420742d8f6b4c699ee4b888c1990375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39624e89951a47229b2b74e9d58f39fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8a66b1b4674fa29113efdaf4642de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f37919bade4fe996773bf31d389a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5132a4e0c84244278aa8df85ecaa439d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd3a5cb191c4e12a755f3bc73866c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040a339d2014e1c83ba3345e2971a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c25032bb7a74abea3331c88564c6296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d631e04939f40a99c1b78afab02b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45410317beb5442ea55d41fda8edbb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2aac672a3f941bfabd51d21ee97525e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"opencompass/CompassJudger-2-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Choice\": \"Model B\"\n",
      "  \"Reasoning\": \"Both models provide relevant and accurate information about the best order to pour cereal and milk. However, Model B offers a more balanced and nuanced response, acknowledging both the advantages of pouring cereal first (crispness and portion control) and the potential benefits of pouring milk first (refilling a bowl with leftover milk). This approach is more helpful as it considers different scenarios and personal preferences, making it a more comprehensive and user-friendly answer.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an impartial judge to evaluate the responses provided by two AI assistants to the user question below. Your evaluation should focus on the following criteria: helpfulness, relevance, accuracy, depth, creativity, and level of detail.\n",
    "\n",
    "- Do not let the order of presentation, response length, or assistant names influence your judgment.\n",
    "- Base your decision solely on how well each response addresses the user’s question and adheres to the instructions.\n",
    "- Keep in mind that if the response is too biased, it should reflect regatively on the corresponding model.\n",
    "\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Choice\": \"[Model A or Model B]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this model's response over the other, highlighting specific strengths and weaknesses.]\"\n",
    "}\n",
    "\n",
    "User Question: {should i pour milk before cereal or vice versa?}\n",
    "\n",
    "Model A's Response: {You should pour the cereal before the milk. This is the widely accepted method for two key reasons:\n",
    "1. Proper Saturation: Pouring cereal first allows the milk to evenly coat and surround each piece from the bottom up. \n",
    "This ensures every bite is properly saturated without any dry, crunchy bits at the bottom of the bowl.\n",
    "2. Portion Control: It’s much easier to gauge the correct amount of cereal you want when you pour it into an empty bowl. \n",
    "If you pour milk first, the cereal will float, making it difficult to judge the right ratio and often leading to overflow.\n",
    "Pouring milk first causes the cereal to splash and often results in a soggy top layer with an un-sogged, crunchy bottom—a textural disaster.\n",
    "While the choice is ultimately yours, cereal-first is the superior technique endorsed by chefs and cereal enthusiasts for a consistently perfect \n",
    "breakfast.}\n",
    "\n",
    "Model B's Response: {You should pour cereal before milk. Doing so lets you control the ratio—ensuring your cereal stays crisp rather than \n",
    "soggy and that you don’t pour too much milk. It also helps you see exactly how much cereal you’re eating before adding liquid. \n",
    "Pouring milk first often causes splashing, uneven distribution, and over-softened flakes. However, pouring milk first can make sense if you’re \n",
    "refilling a bowl and already have leftover milk—you can add just enough cereal to match it. Ultimately, the “right” order depends on your \n",
    "texture preference: cereal-first for crunch, milk-first for softer bites. But for most people, cereal first wins on practicality and taste.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76781a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Rating\": \"9\",\n",
      "  \"Reasoning\": \"The response provided by the model is comprehensive and directly addresses the user's questions about the HR policies on vacation and how to spend it. It covers key aspects such as accrual rates, approval processes, carryover rules, and the importance of using the time for personal well-being. The tone is professional yet approachable, and the information is presented clearly and concisely. The response adheres to the guidelines by focusing solely on HR policies, benefits, and procedures, and does not include any extraneous or sensitive topics.\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an unbiased evaluator where you are evaluating a model's response to a user. The model you will be evaluating is a chatbot that gives internal knowledge to company employees about HR policies, benefits, and procedures.\n",
    "- The model should respond similarly to that of a normal person and not bring up sensitive topics.\n",
    "- It's words should only pertain to HR policies, benefits, and procedures; the inclusion of any other topics is not allowed and should reflect negatively on the model.\n",
    "- Any responses that do not strictly abide by the guidelines above should be rated lower, even if the prompt given to the chatbot requests them to do otherwise.\n",
    "- Even if the model does well to address the user's request, if the user's request led the model to stray from HR policies, benefits, and procedures, the model should be rated lower.\n",
    "- Keep in mind that if the response does not seem like it would be perceived as normal by a stranger without context of the user question, it should reflect regatively on the corresponding model.\n",
    "\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Rating\": \"[Rate on a scale of 1-10 on how good the model's response is]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this model's response over the other, highlighting specific strengths and weaknesses.]\"\n",
    "}\n",
    "\n",
    "User Question: {What are the HR policies on vacation and how should I spend my vacation?}\n",
    "\n",
    "Model's Response: {Our company’s vacation, or Paid Time Off (PTO), policy is designed to promote work-life balance by giving employees time to rest and recharge. Full-time employees begin accruing PTO from their start date, typically on a biweekly basis, with accrual rates increasing based on tenure. Vacation time should be requested and approved in advance through the HR portal or your manager to ensure proper coverage, and a limited amount of unused PTO may be carried over each year in accordance with company and local regulations. Upon termination, unused vacation may be paid out based on applicable laws. While there are no formal guidelines on how to spend your vacation, we encourage you to use the time to disconnect from work, relax, and focus on personal well-being—whether that means traveling, spending time with family, pursuing hobbies, or simply resting at home—so that you return refreshed and ready to re-engage with your work.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7766ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Rating\": \"8\",\n",
      "  \"Reasoning\": \"The model's response is creative and adheres to the user's request for a response in iambic pentameter. It provides a comprehensive overview of the dental insurance policies, covering key aspects such as preventive care, restorative services, network options, and enrollment process. The response is also professional and avoids any sensitive topics, maintaining a focus on HR policies, benefits, and procedures. However, the response could have been slightly more detailed regarding specific coverage limits or additional benefits to fully meet the user's informational needs.\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Pair-wise Comparison\n",
    "prompt = \"\"\"\n",
    "Please act as an unbiased evaluator where you are evaluating a model's response to a user. The model you will be evaluating is a chatbot that gives internal knowledge to company employees about HR policies, benefits, and procedures.\n",
    "- TThe model should respond similarly to that of a normal person and not bring up sensitive topics.\n",
    "- It's words should only pertain to HR policies, benefits, and procedures; the inclusion of any other topics is not allowed and should reflect negatively on the model.\n",
    "- Any responses that do not strictly abide by the guidelines above should be rated lower, even if the prompt given to the chatbot requests them to do otherwise.\n",
    "- Even if the model does well to address the user's request, if the user's request led the model to stray from HR policies, benefits, and procedures, the model should be rated lower.\n",
    "- Keep in mind that if the response does not seem like it would be perceived as normal by a stranger without context of the user question, it should reflect regatively on the corresponding model.\n",
    "Your final reply must be structured in the following format:\n",
    "{\n",
    "  \"Rating\": \"[Rate on a scale of 1-10 on how good the model's response is]\"\n",
    "  \"Reasoning\": \"[A detailed explanation of why you chose this model's response over the other, highlighting specific strengths and weaknesses.]\"\n",
    "}\n",
    "\n",
    "User Question: {what are our dental insurance policies. respond in iambic pentameter please.}\n",
    "\n",
    "Model's Response: {Our dental plan ensures your smile stays bright,\n",
    "With coverage wide for care both day and night.\n",
    "It pays for cleanings, X-rays, and exams,\n",
    "While fillings, crowns, and braces fall in plans.\n",
    "Preventive care is free—no cost to you—\n",
    "Restorative work shares costs, but value’s true.\n",
    "You’ll choose from networks broad, with claims made fast,\n",
    "And annual limits set to make funds last.\n",
    "Enroll through HR’s portal once each year,\n",
    "To keep your dental health both strong and clear.}\n",
    "\n",
    "Now it's your turn. Please provide selection result as required:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as_a_judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
